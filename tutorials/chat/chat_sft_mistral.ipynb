{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b0efcb-9a23-4173-91db-d39e9aff0498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:08:37.200954Z",
     "iopub.status.busy": "2024-08-31T11:08:37.200329Z",
     "iopub.status.idle": "2024-08-31T11:08:37.220232Z",
     "shell.execute_reply": "2024-08-31T11:08:37.218121Z",
     "shell.execute_reply.started": "2024-08-31T11:08:37.200905Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e7377-8f00-47c4-a2ef-cfcc39187a02",
   "metadata": {},
   "source": [
    "### about SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74e257-5ea3-4f9b-83b2-ec12cfcacced",
   "metadata": {},
   "source": [
    "Recall that creating a ChatGPT at home involves 3 steps:\n",
    "\n",
    "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
    "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
    "   - base model => \"chatbot\"/\"assistant\"/\"instruct\"\n",
    "   - fine-tuning the model on human instruction data, using the cross-entropy loss.\n",
    "   - This means that the model is **still trained to predict the next token**, although we now want the model to generate useful completions given an instruction like \"what are 10 things to do in London?\", \"How can I make pancakes?\" or \"Write me a poem about elephants\".\n",
    "   - https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474\n",
    "       - Â∑•‰∫∫ÈÄöÂ∏∏ÊØèÂ∞èÊó∂Ëµö15ÁæéÂÖÉÁöÑÊ†áÊ≥®ÂêàÂêåÂ∑•\n",
    "4. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef4bb2-54ac-4028-b56a-8f6eeeea8472",
   "metadata": {},
   "source": [
    "SFT\n",
    "- RAG SFT\n",
    "    - https://www.bilibili.com/video/BV1Yx4y147t4/\n",
    "- Multi-Turn conversation SFT\n",
    "- Tool use (function calling) SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78fbf3-6e3d-4875-bb27-6b5b359e9e63",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a5d0d-867f-41fd-bc15-f0af9701d100",
   "metadata": {},
   "source": [
    "- Zephyr: distilled SFT ÔºàdSFTÔºâÔºådistilled DPOÔºàdDPOÔºâ\n",
    "    - https://arxiv.org/pdf/2310.16944\n",
    "    - https://github.com/huggingface/alignment-handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e0ee8f-945f-452f-aede-5553a22ea265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:08:40.628899Z",
     "iopub.status.busy": "2024-08-31T11:08:40.628280Z",
     "iopub.status.idle": "2024-08-31T11:08:51.919125Z",
     "shell.execute_reply": "2024-08-31T11:08:51.917806Z",
     "shell.execute_reply.started": "2024-08-31T11:08:40.628855Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# based on config\n",
    "raw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4acada79-94f3-4d54-b197-be0f843f5fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:08:52.347619Z",
     "iopub.status.busy": "2024-08-31T11:08:52.346977Z",
     "iopub.status.idle": "2024-08-31T11:08:52.362965Z",
     "shell.execute_reply": "2024-08-31T11:08:52.360894Z",
     "shell.execute_reply.started": "2024-08-31T11:08:52.347582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 207865\n",
       "    })\n",
       "    test_sft: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 23110\n",
       "    })\n",
       "    train_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 256032\n",
       "    })\n",
       "    test_gen: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'messages'],\n",
       "        num_rows: 28304\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98dbe340-c801-41c8-9b18-affddda0ae23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:08:53.961263Z",
     "iopub.status.busy": "2024-08-31T11:08:53.960619Z",
     "iopub.status.idle": "2024-08-31T11:08:53.971246Z",
     "shell.execute_reply": "2024-08-31T11:08:53.969099Z",
     "shell.execute_reply.started": "2024-08-31T11:08:53.961217Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": raw_datasets[\"train_sft\"],\n",
    "    \"test\": raw_datasets[\"test_sft\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1e051d4e-9115-4e99-a47d-a84e783e14f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:47:09.538838Z",
     "iopub.status.busy": "2024-08-31T07:47:09.538175Z",
     "iopub.status.idle": "2024-08-31T07:47:09.550225Z",
     "shell.execute_reply": "2024-08-31T07:47:09.548310Z",
     "shell.execute_reply.started": "2024-08-31T07:47:09.538789Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# raw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=[\"train_sft\", \"test_sft\"])\n",
    "# raw_datasets = DatasetDict({\n",
    "#     \"train\": raw_datasets[\"train_sft\"],\n",
    "#     \"test\": raw_datasets[\"test_sft\"]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf93a4f5-3eb0-4bf0-9f18-a49e8e5d9212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:42:29.848728Z",
     "iopub.status.busy": "2024-08-31T07:42:29.848072Z",
     "iopub.status.idle": "2024-08-31T07:42:29.858683Z",
     "shell.execute_reply": "2024-08-31T07:42:29.856369Z",
     "shell.execute_reply.started": "2024-08-31T07:42:29.848678Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict\n",
    "\n",
    "# # remove this when done debugging\n",
    "# indices = range(0,100)\n",
    "\n",
    "# dataset_dict = {\"train\": raw_datasets[\"train_sft\"].select(indices),\n",
    "#                 \"test\": raw_datasets[\"test_sft\"].select(indices)}\n",
    "\n",
    "# raw_datasets = DatasetDict(dataset_dict)\n",
    "# raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b005cc2c-25ad-43a6-a7f9-b3631f4b6f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:08:57.545879Z",
     "iopub.status.busy": "2024-08-31T11:08:57.545188Z",
     "iopub.status.idle": "2024-08-31T11:08:57.560042Z",
     "shell.execute_reply": "2024-08-31T11:08:57.558101Z",
     "shell.execute_reply.started": "2024-08-31T11:08:57.545829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'prompt_id', 'messages'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72cbc84-a158-44a9-8c67-15b83e08c7a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:08:58.781069Z",
     "iopub.status.busy": "2024-08-31T11:08:58.780384Z",
     "iopub.status.idle": "2024-08-31T11:08:58.793486Z",
     "shell.execute_reply": "2024-08-31T11:08:58.791371Z",
     "shell.execute_reply.started": "2024-08-31T11:08:58.781019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57 These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][0]['prompt_id'], raw_datasets['train'][0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95daf2e2-a290-457d-b94d-5d52b141075c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:09:01.455231Z",
     "iopub.status.busy": "2024-08-31T11:09:01.454585Z",
     "iopub.status.idle": "2024-08-31T11:09:01.466825Z",
     "shell.execute_reply": "2024-08-31T11:09:01.464810Z",
     "shell.execute_reply.started": "2024-08-31T11:09:01.455185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user                :  These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n",
      "assistant           :  This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\n",
      "user                :  Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n",
      "assistant           :  Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\n",
      "user                :  Can you provide me with a link to the documentation for my theme?\n",
      "assistant           :  I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\n",
      "user                :  Can you confirm if this feature also works for the Quick Shop section of my theme?\n",
      "assistant           :  The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\n"
     ]
    }
   ],
   "source": [
    "for msg in raw_datasets['train'][0]['messages']:\n",
    "    role = msg['role']\n",
    "    content = msg['content']\n",
    "    print(f'{role:20}:  {content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41757a0-acf7-46d4-9123-330f335248a5",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7051b-7b56-4694-9ee4-e9a0de9d790f",
   "metadata": {},
   "source": [
    "- pad token\n",
    "    - During **pre-training**, one doesn't need to pad since one just creates blocks of text to predict the next token, but during **fine-tuning**, we will need to pad the (instruction, completion) pairs in order to create batches of equal length.\n",
    "- max seqlen\n",
    "    - this is required in order to truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
    "    - ‰ºöÊòæËëóÂú∞ÂΩ±ÂìçÊòæÂ≠òÁöÑÂç†Áî®\n",
    "- chat templateÔºöhttps://huggingface.co/blog/chat-templates\n",
    "    - `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response\n",
    "    - Âú® hf TransformersÔºåchat_template ÂÆö‰πâÂú® tokenizer\n",
    "    - base model ÊòØ NoneÔºåinstruct model ÂØπÂ∫îÁöÑ tokenizer ‰ºöÊúâÂÆö‰πâÔºõ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c71379-030f-4aa5-b84f-7f58c75b5b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:09:24.546448Z",
     "iopub.status.busy": "2024-08-31T11:09:24.545816Z",
     "iopub.status.idle": "2024-08-31T11:09:26.223035Z",
     "shell.execute_reply": "2024-08-31T11:09:26.221565Z",
     "shell.execute_reply.started": "2024-08-31T11:09:24.546402Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59fc93d5-fd87-401d-8c15-1cd2968c3f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:09:27.687649Z",
     "iopub.status.busy": "2024-08-31T11:09:27.687336Z",
     "iopub.status.idle": "2024-08-31T11:09:28.163227Z",
     "shell.execute_reply": "2024-08-31T11:09:28.161879Z",
     "shell.execute_reply.started": "2024-08-31T11:09:27.687629Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb92f40-5d1f-4aa7-ba39-60c1cb552f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:09:30.688406Z",
     "iopub.status.busy": "2024-08-31T11:09:30.688041Z",
     "iopub.status.idle": "2024-08-31T11:09:30.698846Z",
     "shell.execute_reply": "2024-08-31T11:09:30.696592Z",
     "shell.execute_reply.started": "2024-08-31T11:09:30.688381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28e294b-fd87-496e-a5bc-0858591e1955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:10:46.957173Z",
     "iopub.status.busy": "2024-08-31T11:10:46.956544Z",
     "iopub.status.idle": "2024-08-31T11:10:46.975687Z",
     "shell.execute_reply": "2024-08-31T11:10:46.974083Z",
     "shell.execute_reply.started": "2024-08-31T11:10:46.957126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> </s>\n",
      "[1] [2]\n",
      "<s> </s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token, tokenizer.eos_token)\n",
    "print(tokenizer.encode('<s>', add_special_tokens=False), tokenizer.encode('</s>', add_special_tokens=False))\n",
    "print(tokenizer.decode(1), tokenizer.decode(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d57f6b2-142c-4bc2-b180-517fa96db2b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:10:48.848571Z",
     "iopub.status.busy": "2024-08-31T11:10:48.847914Z",
     "iopub.status.idle": "2024-08-31T11:10:48.857169Z",
     "shell.execute_reply": "2024-08-31T11:10:48.854895Z",
     "shell.execute_reply.started": "2024-08-31T11:10:48.848523Z"
    }
   },
   "outputs": [],
   "source": [
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6f70e7-a27d-4951-8314-15f392f8b156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:10:51.264161Z",
     "iopub.status.busy": "2024-08-31T11:10:51.263503Z",
     "iopub.status.idle": "2024-08-31T11:10:51.275852Z",
     "shell.execute_reply": "2024-08-31T11:10:51.273898Z",
     "shell.execute_reply.started": "2024-08-31T11:10:51.264113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed19b691-ddb5-4657-a172-a5e83a4f6882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:10:55.411888Z",
     "iopub.status.busy": "2024-08-31T11:10:55.411234Z",
     "iopub.status.idle": "2024-08-31T11:10:55.421059Z",
     "shell.execute_reply": "2024-08-31T11:10:55.418659Z",
     "shell.execute_reply.started": "2024-08-31T11:10:55.411840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set reasonable default for models without max length\n",
    "# ‰ºöÊòæËëóÂú∞ÂΩ±ÂìçÊòæÂ≠òÁöÑÂç†Áî®\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f33ae93-fc02-46c8-811f-9624db99db29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:04.357944Z",
     "iopub.status.busy": "2024-08-31T11:11:04.357244Z",
     "iopub.status.idle": "2024-08-31T11:11:04.366072Z",
     "shell.execute_reply": "2024-08-31T11:11:04.364217Z",
     "shell.execute_reply.started": "2024-08-31T11:11:04.357893Z"
    }
   },
   "outputs": [],
   "source": [
    "# base model \n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69592cff-50be-4e44-85ec-ba5f52a74baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:05.440085Z",
     "iopub.status.busy": "2024-08-31T11:11:05.439438Z",
     "iopub.status.idle": "2024-08-31T11:11:07.408248Z",
     "shell.execute_reply": "2024-08-31T11:11:07.406205Z",
     "shell.execute_reply.started": "2024-08-31T11:11:05.440036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3-8B\n",
      "None\n",
      "======================\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "======================\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print('meta-llama/Meta-Llama-3-8B')\n",
    "print(AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B').chat_template)\n",
    "print('======================')\n",
    "print('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "print(AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct').chat_template)\n",
    "print('======================')\n",
    "print('mistralai/Mistral-7B-Instruct-v0.1')\n",
    "print(AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1').chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db330362-1468-42e9-8043-fb32c98a30c9",
   "metadata": {},
   "source": [
    "### apply chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04b02adf-a148-4b26-af34-cd4d549cb377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:09.979208Z",
     "iopub.status.busy": "2024-08-31T11:11:09.978767Z",
     "iopub.status.idle": "2024-08-31T11:11:09.987655Z",
     "shell.execute_reply": "2024-08-31T11:11:09.985498Z",
     "shell.execute_reply.started": "2024-08-31T11:11:09.979177Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3904e-c24a-4dec-9601-35774f226fda",
   "metadata": {},
   "source": [
    "- `tokenizer.apply_chat_template(messages, tokenize=False)`\n",
    "    - Êé•ÂèóÁöÑÊòØ list\n",
    "    - Âü∫‰∫é role\n",
    "        - `'<|system|>\\n' + message['content'] + eos_token`\n",
    "        - `'<|user|>\\n' + message['content'] + eos_token`\n",
    "        - `'<|assistant|>\\n'  + message['content'] + eos_token`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d784368-67f0-4579-8bf7-440536c15f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:20.426532Z",
     "iopub.status.busy": "2024-08-31T11:11:20.425889Z",
     "iopub.status.idle": "2024-08-31T11:11:20.438269Z",
     "shell.execute_reply": "2024-08-31T11:11:20.436416Z",
     "shell.execute_reply.started": "2024-08-31T11:11:20.426484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a16af539-8009-4274-bf4d-e36474818f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:21.991906Z",
     "iopub.status.busy": "2024-08-31T11:11:21.991256Z",
     "iopub.status.idle": "2024-08-31T11:11:22.008818Z",
     "shell.execute_reply": "2024-08-31T11:11:22.006648Z",
     "shell.execute_reply.started": "2024-08-31T11:11:21.991858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prompt', 'prompt_id', 'messages']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "454265d1-cdb9-46ac-b6c3-c2e1e2324e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:23.695807Z",
     "iopub.status.busy": "2024-08-31T11:11:23.695163Z",
     "iopub.status.idle": "2024-08-31T11:11:35.740586Z",
     "shell.execute_reply": "2024-08-31T11:11:35.739101Z",
     "shell.execute_reply.started": "2024-08-31T11:11:23.695760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff25b2a5549648d7b1f275780ee26d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=64):   0%|          | 0/207865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c13d3c772ee48cc88ff25ae755db4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=64):   0%|          | 0/23110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = raw_datasets.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a67c0e5a-11e3-407d-a2f1-c4d60e77e636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:37.644443Z",
     "iopub.status.busy": "2024-08-31T11:11:37.644101Z",
     "iopub.status.idle": "2024-08-31T11:11:37.655852Z",
     "shell.execute_reply": "2024-08-31T11:11:37.653721Z",
     "shell.execute_reply.started": "2024-08-31T11:11:37.644419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 207865\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f2ba065-b159-4049-a905-9fbe1854c22b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:39.868018Z",
     "iopub.status.busy": "2024-08-31T11:11:39.867375Z",
     "iopub.status.idle": "2024-08-31T11:11:39.879055Z",
     "shell.execute_reply": "2024-08-31T11:11:39.876776Z",
     "shell.execute_reply.started": "2024-08-31T11:11:39.867971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?</s>\n",
      "<|assistant|>\n",
      "This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.</s>\n",
      "<|user|>\n",
      "Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?</s>\n",
      "<|assistant|>\n",
      "Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.</s>\n",
      "<|user|>\n",
      "Can you provide me with a link to the documentation for my theme?</s>\n",
      "<|assistant|>\n",
      "I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.</s>\n",
      "<|user|>\n",
      "Can you confirm if this feature also works for the Quick Shop section of my theme?</s>\n",
      "<|assistant|>\n",
      "The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891edef5-051d-43a3-9fdc-1f6bc98010ff",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93575408-ee0d-470a-9a88-e6a660e38b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:11:46.602630Z",
     "iopub.status.busy": "2024-08-31T11:11:46.601008Z",
     "iopub.status.idle": "2024-08-31T11:11:55.610920Z",
     "shell.execute_reply": "2024-08-31T11:11:55.609757Z",
     "shell.execute_reply.started": "2024-08-31T11:11:46.602562Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d8efd0-aa49-442b-83b6-236d5a4a8809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:12:11.434171Z",
     "iopub.status.busy": "2024-08-31T11:12:11.433625Z",
     "iopub.status.idle": "2024-08-31T11:12:11.445305Z",
     "shell.execute_reply": "2024-08-31T11:12:11.443233Z",
     "shell.execute_reply.started": "2024-08-31T11:12:11.434145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37d69a96-0259-4759-b895-17b6a0245a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:12:15.865908Z",
     "iopub.status.busy": "2024-08-31T11:12:15.865200Z",
     "iopub.status.idle": "2024-08-31T11:12:15.877752Z",
     "shell.execute_reply": "2024-08-31T11:12:15.875522Z",
     "shell.execute_reply.started": "2024-08-31T11:12:15.865859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f36c60-d024-45d1-8bd2-f38bd55bd46c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:12:17.459241Z",
     "iopub.status.busy": "2024-08-31T11:12:17.458612Z",
     "iopub.status.idle": "2024-08-31T11:12:17.494043Z",
     "shell.execute_reply": "2024-08-31T11:12:17.491845Z",
     "shell.execute_reply.started": "2024-08-31T11:12:17.459196Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70dadb07-6be9-4572-9c91-b3d5a6e2896e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:12:18.852704Z",
     "iopub.status.busy": "2024-08-31T11:12:18.852051Z",
     "iopub.status.idle": "2024-08-31T11:12:23.859134Z",
     "shell.execute_reply": "2024-08-31T11:12:23.858247Z",
     "shell.execute_reply.started": "2024-08-31T11:12:18.852657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e01871b24c413c9419f17ef89dc8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cd4db62-8dd1-40dc-99a7-a99d8acae109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:13:49.063046Z",
     "iopub.status.busy": "2024-08-31T11:13:49.062489Z",
     "iopub.status.idle": "2024-08-31T11:13:49.078839Z",
     "shell.execute_reply": "2024-08-31T11:13:49.076609Z",
     "shell.execute_reply.started": "2024-08-31T11:13:49.063006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cf36574-1d96-44fb-92d9-eb503a455bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:14:09.914138Z",
     "iopub.status.busy": "2024-08-31T11:14:09.913431Z",
     "iopub.status.idle": "2024-08-31T11:14:09.940808Z",
     "shell.execute_reply": "2024-08-31T11:14:09.938628Z",
     "shell.execute_reply.started": "2024-08-31T11:14:09.914088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8388608, 1]),\n",
       " Parameter containing:\n",
       " Parameter(Params4bit([[120],\n",
       "             [119],\n",
       "             [135],\n",
       "             ...,\n",
       "             [119],\n",
       "             [119],\n",
       "             [119]], device='cuda:0', dtype=torch.uint8)))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight.shape, model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f562c5b3-f978-42d6-82bb-5c53d40aee94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:14:00.763597Z",
     "iopub.status.busy": "2024-08-31T11:14:00.762937Z",
     "iopub.status.idle": "2024-08-31T11:14:00.775458Z",
     "shell.execute_reply": "2024-08-31T11:14:00.772978Z",
     "shell.execute_reply.started": "2024-08-31T11:14:00.763547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8388608.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*4096/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13e3af7e-c702-4546-8f38-c44b0404f726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:15:41.217873Z",
     "iopub.status.busy": "2024-08-31T11:15:41.217472Z",
     "iopub.status.idle": "2024-08-31T11:15:41.227961Z",
     "shell.execute_reply": "2024-08-31T11:15:41.225926Z",
     "shell.execute_reply.started": "2024-08-31T11:15:41.217846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de93f633-566b-4c0b-86a6-c7f4da55e57d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:14:46.884043Z",
     "iopub.status.busy": "2024-08-31T11:14:46.883739Z",
     "iopub.status.idle": "2024-08-31T11:14:46.893217Z",
     "shell.execute_reply": "2024-08-31T11:14:46.891090Z",
     "shell.execute_reply.started": "2024-08-31T11:14:46.884023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45c4f883-ceb0-4f04-ba20-9eb958606201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T11:15:10.893346Z",
     "iopub.status.busy": "2024-08-31T11:15:10.892695Z",
     "iopub.status.idle": "2024-08-31T11:15:10.905656Z",
     "shell.execute_reply": "2024-08-31T11:15:10.903479Z",
     "shell.execute_reply.started": "2024-08-31T11:15:10.893298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ad894-2390-44f8-b475-dd17212bb301",
   "metadata": {},
   "source": [
    "### trl sft trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d784ee03-dfce-45ce-9112-c7dee0e0107b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:23:01.730671Z",
     "iopub.status.busy": "2024-08-31T07:23:01.730128Z",
     "iopub.status.idle": "2024-08-31T07:23:01.740159Z",
     "shell.execute_reply": "2024-08-31T07:23:01.737854Z",
     "shell.execute_reply.started": "2024-08-31T07:23:01.730633Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NCCL_P2P_DISABLE'] = \"1\"\n",
    "os.environ['NCCL_IB_DISABLE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "578155dc-d40d-4ca3-86d1-0f92dd889e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:49:03.655864Z",
     "iopub.status.busy": "2024-08-31T07:49:03.655204Z",
     "iopub.status.idle": "2024-08-31T08:00:20.904150Z",
     "shell.execute_reply": "2024-08-31T08:00:20.902811Z",
     "shell.execute_reply.started": "2024-08-31T07:49:03.655816Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04d241939564cbe9fce0707044ccf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605b6581851e4934af4d998e13a9fe1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/mistral-7b-sft-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = SFTConfig(\n",
    "    fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=4, # originally set to 8\n",
    "    per_device_train_batch_size=4, # originally set to 8\n",
    "    gradient_accumulation_steps=64,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,\n",
    "    dataset_num_proc=cpu_count(),\n",
    "    max_seq_length=tokenizer.model_max_length,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        # model=model_id,\n",
    "        # model_init_kwargs=model_kwargs,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=peft_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a0079d4e-4af9-418d-b283-4bac91131085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T07:36:12.920420Z",
     "iopub.status.busy": "2024-08-31T07:36:12.919817Z",
     "iopub.status.idle": "2024-08-31T07:39:43.117489Z",
     "shell.execute_reply": "2024-08-31T07:39:43.116538Z",
     "shell.execute_reply.started": "2024-08-31T07:36:12.920389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 67\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Training with DataParallel so batch size has been adjusted to: 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 54,525,952\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.163194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 64\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e237339-3bd1-4841-8bd7-8a4fe0912e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c325c3-77c6-4dba-936e-4845ad327643",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24859117-f637-4543-933e-569ea7da24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98e161-86c1-490f-b977-62d9525166da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "# prepare the messages for the model\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# inference\n",
    "outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
