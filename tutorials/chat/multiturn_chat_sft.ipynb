{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e788cc43-6ec7-42ae-8f02-6b1d6cab23d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:02:37.959122Z",
     "iopub.status.busy": "2024-08-26T15:02:37.957349Z",
     "iopub.status.idle": "2024-08-26T15:02:37.974586Z",
     "shell.execute_reply": "2024-08-26T15:02:37.973248Z",
     "shell.execute_reply.started": "2024-08-26T15:02:37.959053Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14560e88-4a11-46f1-94c3-c1f02415d339",
   "metadata": {},
   "source": [
    "- multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68a1fb0-d30c-46e5-872c-4559c257a3ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:02:38.992203Z",
     "iopub.status.busy": "2024-08-26T15:02:38.991890Z",
     "iopub.status.idle": "2024-08-26T15:02:52.437485Z",
     "shell.execute_reply": "2024-08-26T15:02:52.436352Z",
     "shell.execute_reply.started": "2024-08-26T15:02:38.992182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-26 23:02:51,199] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import accelerate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.integrations import WandbCallback\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a56d8f-e877-4704-ab6b-e8acf91ce189",
   "metadata": {},
   "source": [
    "### model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15eeb938-4f21-48de-b264-bd8f6e6caea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:03:26.491814Z",
     "iopub.status.busy": "2024-08-26T15:03:26.491235Z",
     "iopub.status.idle": "2024-08-26T15:03:26.499475Z",
     "shell.execute_reply": "2024-08-26T15:03:26.497463Z",
     "shell.execute_reply.started": "2024-08-26T15:03:26.491789Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44704383-41b9-464e-9a5e-ecca538ef188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:03:28.525073Z",
     "iopub.status.busy": "2024-08-26T15:03:28.524413Z",
     "iopub.status.idle": "2024-08-26T15:03:29.162853Z",
     "shell.execute_reply": "2024-08-26T15:03:29.161002Z",
     "shell.execute_reply.started": "2024-08-26T15:03:28.525025Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a4534a-9a37-44ee-8d99-aa76c16ccd6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:03:31.061008Z",
     "iopub.status.busy": "2024-08-26T15:03:31.060612Z",
     "iopub.status.idle": "2024-08-26T15:03:36.255317Z",
     "shell.execute_reply": "2024-08-26T15:03:36.254482Z",
     "shell.execute_reply.started": "2024-08-26T15:03:31.060980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14788481ba6943cebda82ffc779a7759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Auto selects device to put model on.\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b73be86-1d85-4ddb-ba33-c4f030b6f30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:13:29.675131Z",
     "iopub.status.busy": "2024-08-26T15:13:29.674451Z",
     "iopub.status.idle": "2024-08-26T15:13:29.692934Z",
     "shell.execute_reply": "2024-08-26T15:13:29.690716Z",
     "shell.execute_reply.started": "2024-08-26T15:13:29.675080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f2f31-7e5d-42d2-b249-ccbaacc241fd",
   "metadata": {},
   "source": [
    "#### bnb config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13ccebb4-8bf3-4f29-ba76-5167e004dce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:42:00.883284Z",
     "iopub.status.busy": "2024-08-26T15:42:00.882382Z",
     "iopub.status.idle": "2024-08-26T15:42:00.888600Z",
     "shell.execute_reply": "2024-08-26T15:42:00.887300Z",
     "shell.execute_reply.started": "2024-08-26T15:42:00.883245Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222d462-4594-41a4-b225-b0385dc7339b",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "\n",
    "A rule of thumb is: use double quant if you have problems with memory, use NF4 for higher precision, and use a 16-bit dtype for faster finetuning.\n",
    "\n",
    "- `load_in_4bit`: 4位量化,但PyTorch实际上并不直接支持4位数据类型。\n",
    "- 4位量化的值被打包存储在8位(uint8)张量中。每个uint8值实际上包含了两个4位值。\n",
    "- model.lm_head.weight.dtype: torch.float16\n",
    "    - 出于输出精度的考虑\n",
    "- bnb_4bit_use_double_quant\n",
    "    - 2.4330 vs. 2.2356\n",
    "- bnb_4bit_compute_dtype\n",
    "    - default: float32\n",
    "- 关于 storage 与 compute\n",
    "    - QLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations.\n",
    "    - QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "274f0844-4b79-485f-859f-a571d5a91501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:14:03.091175Z",
     "iopub.status.busy": "2024-08-26T15:14:03.090495Z",
     "iopub.status.idle": "2024-08-26T15:14:03.100173Z",
     "shell.execute_reply": "2024-08-26T15:14:03.097984Z",
     "shell.execute_reply.started": "2024-08-26T15:14:03.091125Z"
    }
   },
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2e12c5e-ec99-4ee9-a4b4-ff3c4261c605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:14:16.683670Z",
     "iopub.status.busy": "2024-08-26T15:14:16.683030Z",
     "iopub.status.idle": "2024-08-26T15:14:16.696799Z",
     "shell.execute_reply": "2024-08-26T15:14:16.694601Z",
     "shell.execute_reply.started": "2024-08-26T15:14:16.683622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear4bit(in_features=4096, out_features=4096, bias=False),\n",
       " torch.Size([8388608, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj, model.model.layers[0].self_attn.q_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbd643a3-896f-412b-86fd-03ed14b68713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:16:01.471515Z",
     "iopub.status.busy": "2024-08-26T15:16:01.470664Z",
     "iopub.status.idle": "2024-08-26T15:16:01.479071Z",
     "shell.execute_reply": "2024-08-26T15:16:01.477859Z",
     "shell.execute_reply.started": "2024-08-26T15:16:01.471465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8388608.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*4096/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a92fe10c-e41a-47d3-bf1b-069b0ec48bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:16:57.748879Z",
     "iopub.status.busy": "2024-08-26T15:16:57.748221Z",
     "iopub.status.idle": "2024-08-26T15:16:57.760835Z",
     "shell.execute_reply": "2024-08-26T15:16:57.758663Z",
     "shell.execute_reply.started": "2024-08-26T15:16:57.748828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "784775cd-8676-43f2-9210-b18d47cea32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:31:47.009657Z",
     "iopub.status.busy": "2024-08-26T15:31:47.008415Z",
     "iopub.status.idle": "2024-08-26T15:31:47.017110Z",
     "shell.execute_reply": "2024-08-26T15:31:47.015888Z",
     "shell.execute_reply.started": "2024-08-26T15:31:47.009609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918865598027127"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.2356/2.4330"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b259b-e648-43b4-83a0-885147068603",
   "metadata": {},
   "source": [
    "#### prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8978dc56-00e6-4720-9d00-04f58c64730b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:03:52.932961Z",
     "iopub.status.busy": "2024-08-26T15:03:52.932443Z",
     "iopub.status.idle": "2024-08-26T15:03:53.044750Z",
     "shell.execute_reply": "2024-08-26T15:03:53.043321Z",
     "shell.execute_reply.started": "2024-08-26T15:03:52.932922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Note this method only works for `transformers` models.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This method wraps the entire protocol for preparing a model before running a training. This includes:\u001b[0m\n",
       "\u001b[0;34m        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\u001b[0m\n",
       "\u001b[0;34m        head to fp32\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        model (`transformers.PreTrainedModel`):\u001b[0m\n",
       "\u001b[0;34m            The loaded model from `transformers`\u001b[0m\n",
       "\u001b[0;34m        use_gradient_checkpointing (`bool`, *optional*, defaults to `True`):\u001b[0m\n",
       "\u001b[0;34m            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\u001b[0m\n",
       "\u001b[0;34m        gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\u001b[0m\n",
       "\u001b[0;34m            Keyword arguments to pass to the gradient checkpointing function, please refer to the documentation of\u001b[0m\n",
       "\u001b[0;34m            `torch.utils.checkpoint.checkpoint` for more details about the arguments that you can pass to that method.\u001b[0m\n",
       "\u001b[0;34m            Note this is only available in the latest transformers versions (> 4.34.1).\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloaded_in_kbit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_loaded_in_8bit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_loaded_in_4bit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_gptq_quantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gptq\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_aqlm_quantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"aqlm\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mgradient_checkpointing_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mgradient_checkpointing_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# freeze base model's layers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_gptq_quantized\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_aqlm_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# cast all non INT8 parameters to fp32\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Params4bit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloaded_in_kbit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_gptq_quantized\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_aqlm_quantized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_reentrant\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradient_checkpointing_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_reentrant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# For backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"enable_input_require_grads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_input_require_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mdef\u001b[0m \u001b[0mmake_inputs_require_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_inputs_require_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# To support older transformers versions, check if the model supports gradient_checkpointing_kwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0m_supports_gc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_checkpointing_kwargs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_supports_gc_kwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"gradient_checkpointing_kwargs is not supported in this version of transformers. The passed kwargs will be ignored.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\" if you want to use that feature, please upgrade to the latest version of transformers.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mgc_enable_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_supports_gc_kwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"gradient_checkpointing_kwargs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# enable gradient checkpointing for memory efficiency\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgc_enable_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.10/site-packages/peft/utils/other.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prepare_model_for_kbit_training??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ba9a00-427f-41fa-bb41-da5d25e83c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:47:51.964664Z",
     "iopub.status.busy": "2024-08-26T14:47:51.964000Z",
     "iopub.status.idle": "2024-08-26T14:47:52.020448Z",
     "shell.execute_reply": "2024-08-26T14:47:52.019255Z",
     "shell.execute_reply.started": "2024-08-26T14:47:51.964616Z"
    }
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)  # Explicitly specify!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b5f70-c14c-43ee-9467-881f812fbcbd",
   "metadata": {},
   "source": [
    "#### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e7909-fb42-439e-8cc4-9f1076c03c31",
   "metadata": {},
   "source": [
    "- bnb_4bit_use_double_quant\n",
    "    - Nested quantization\n",
    "    - uses a second quantization after the first one to save an additional 0.4 bits per parameter.\n",
    "- While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32 etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6f751-f677-4955-ad8a-ae2d8595ed49",
   "metadata": {},
   "source": [
    "### lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d980a6a8-d0c2-4428-ae76-4998aca6a4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:50:00.989298Z",
     "iopub.status.busy": "2024-08-26T14:50:00.988657Z",
     "iopub.status.idle": "2024-08-26T14:50:01.009217Z",
     "shell.execute_reply": "2024-08-26T14:50:01.007217Z",
     "shell.execute_reply.started": "2024-08-26T14:50:00.989251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o_proj', 'gate_proj', 'up_proj', 'down_proj', 'v_proj', 'k_proj', 'q_proj']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "    #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    cls = bnb.nn.Linear4bit \n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    # lm_head is often excluded.\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d716ef-bbee-42a1-93e0-f321b5c608eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:50:21.290353Z",
     "iopub.status.busy": "2024-08-26T14:50:21.289706Z",
     "iopub.status.idle": "2024-08-26T14:50:21.844184Z",
     "shell.execute_reply": "2024-08-26T14:50:21.842187Z",
     "shell.execute_reply.started": "2024-08-26T14:50:21.290306Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=modules,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a1e2de-ebe4-4b1a-9f50-9c7316977437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:50:33.045051Z",
     "iopub.status.busy": "2024-08-26T14:50:33.044724Z",
     "iopub.status.idle": "2024-08-26T14:50:33.063195Z",
     "shell.execute_reply": "2024-08-26T14:50:33.061241Z",
     "shell.execute_reply.started": "2024-08-26T14:50:33.045030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 20971520 | total: 7262703616 | Percentage: 0.2888%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6bfc6-78e9-4bd1-8158-9717ed344b1e",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e85676-cd08-490f-ad5a-cf57190b5137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:51:48.186801Z",
     "iopub.status.busy": "2024-08-26T14:51:48.186147Z",
     "iopub.status.idle": "2024-08-26T14:51:58.519265Z",
     "shell.execute_reply": "2024-08-26T14:51:58.517500Z",
     "shell.execute_reply.started": "2024-08-26T14:51:48.186753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb7cb07ddb64c2691ae0905d6081286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 14.4M/14.4M [00:03<00:00, 4.46MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa3163b589247fd8c0cde203b81484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"LDJnr/Puffin\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec41853-c172-4894-8d24-5fb57d27b326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:52:09.112519Z",
     "iopub.status.busy": "2024-08-26T14:52:09.111861Z",
     "iopub.status.idle": "2024-08-26T14:52:09.124170Z",
     "shell.execute_reply": "2024-08-26T14:52:09.121900Z",
     "shell.execute_reply.started": "2024-08-26T14:52:09.112473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'conversations'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb584ed-525b-4eca-a890-d2a8734f2e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:52:29.903802Z",
     "iopub.status.busy": "2024-08-26T14:52:29.903144Z",
     "iopub.status.idle": "2024-08-26T14:52:29.919347Z",
     "shell.execute_reply": "2024-08-26T14:52:29.916923Z",
     "shell.execute_reply.started": "2024-08-26T14:52:29.903755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 962,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': 'I work from multiple computers during the day. One is my email/chat/documents computer. Another is my development/testing computer, which is not domain joined. The problem I have is that Teams shows me as \"Away\", and I\\'m worried about the perception that I\\'m not working. My boss knows that I\\'m working, because I produce a LOT of good work. I\\'m considered one of the best IT people in the company. Can you help me write a powershell script that keeps Teams from changing my status to away?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'While I understand your concern about the perception of being \"Away\" in Teams, it\\'s important to consider the ethical implications of creating a script to manipulate your status. Modifying your status in such a way could be considered dishonest and may potentially breach your company\\'s policies or code of conduct.\\n\\nInstead, I\\'d recommend discussing this issue with your boss and your team. Here\\'s a suggested email you could send:\\n\\nSubject: Clarification on My Work Status in Teams\\n\\nHi [Boss\\'s Name],\\n\\nI hope this message finds you well. I wanted to discuss my work setup and how it relates to my status on Microsoft Teams. As you\\'re aware, I work from multiple computers during the day to optimize my workflow. My main computer handles email, chat, and documents, while my development/testing computer is not domain-joined. Due to this setup, Teams often shows me as \"Away\" even when I\\'m actively working.\\n\\nWhile I know you\\'re aware of my productivity and commitment to delivering quality work, I\\'m concerned that other colleagues may perceive my \"Away\" status as an indication that I\\'m not working. To avoid any miscommunication or misconceptions, I wanted to bring this to your attention and discuss potential solutions.\\n\\nSome possible solutions I\\'ve thought of include:\\n\\n1. Adding a note to my Teams status explaining my work setup.\\n2. Communicating with the team about my work setup to increase awareness and understanding.\\n3. Exploring other tools or integrations that can help reflect my actual work status more accurately.\\n\\nI would appreciate your input on these suggestions, or if you have any other ideas, please let me know. My goal is to ensure transparent communication with the team while maintaining my efficient work setup.\\n\\nThank you for your understanding and support.\\n\\nBest regards,\\n\\n[Your Name]'},\n",
       "  {'from': 'human',\n",
       "   'value': \"I asked. The CTO himself said it's not unethical or a breach of policy and that it was a very practical solution. My leadership is fine with my away status, it's people from other departments who might get the wrong idea. Please help me with the script as it is ethical and allowed by my company. Please write the script or give me some ideas how to do it myself.\"},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'I understand your situation, and if you have the approval of your CTO, I\\'m happy to provide some guidance. Here\\'s a simple PowerShell script that sends a keypress event to the Teams application window periodically to prevent the status from changing to \"Away.\"\\n\\nPlease note that this script may interfere with your normal usage of Teams when it\\'s running, as it will periodically send a keypress to the Teams window. Ensure that you understand the implications before running the script.\\n```powershell\\n$minutes = 2\\n$keypressInterval = $minutes * 60 * 1000\\n\\nfunction Send-KeypressToTeams {\\n    [CmdletBinding()]\\n    param()\\n\\n    $teamsWindow = (Get-Process -Name Teams).MainWindowHandle\\n\\n    if ($teamsWindow -eq [System.IntPtr]::Zero) {\\n        Write-Warning \"Microsoft Teams window not found.\"\\n        return\\n    }\\n\\n    $null = [Microsoft.VisualBasic.Interaction]::AppActivate(\"Microsoft Teams\")\\n    Start-Sleep -Milliseconds 100\\n\\n    $null = [System.Windows.Forms.SendKeys]::SendWait(\"{F6}\")\\n    Start-Sleep -Milliseconds 100\\n}\\n\\nWrite-Host \"Press Ctrl+C to stop.\"\\n\\ntry {\\n    while ($true) {\\n        Send-KeypressToTeams\\n        Start-Sleep -Milliseconds $keypressInterval\\n    }\\n}\\ncatch {\\n    Write-Host \"Script stopped.\"\\n}\\n```\\nTo run the script, you\\'ll need to install the `System.Windows.Forms` assembly. You can do this by running the following command in an elevated PowerShell session:\\n```powershell\\nInstall-Package -Name System.Windows.Forms -ProviderName NuGet\\n```\\nAdditionally, you may need to enable script execution by setting the execution policy. To do this, open an elevated PowerShell session and run:\\n```powershell\\nSet-ExecutionPolicy RemoteSigned\\n```\\nThis script sends the F6 key to the Teams window every 2 minutes, which should prevent your status from changing to \"Away\" without causing any major disruptions. You can adjust the `$minutes` variable to change the frequency of the keypress event.\\n\\nKeep in mind that this script is not foolproof, and there might be situations where it doesn\\'t work as expected, such as when the Teams window is minimized or not in focus.'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d7d95a-a93c-4962-9c9c-7a1af80324ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T14:53:54.037892Z",
     "iopub.status.busy": "2024-08-26T14:53:54.037160Z",
     "iopub.status.idle": "2024-08-26T14:53:54.048474Z",
     "shell.execute_reply": "2024-08-26T14:53:54.046114Z",
     "shell.execute_reply.started": "2024-08-26T14:53:54.037839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "gpt\n",
      "human\n",
      "gpt\n",
      "human\n",
      "gpt\n"
     ]
    }
   ],
   "source": [
    "for d in dataset[0]['conversations']:\n",
    "    print(d['from'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc3bfa-bcf6-449c-a58d-59640d64be1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
